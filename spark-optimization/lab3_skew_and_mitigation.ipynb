{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc5a516",
   "metadata": {},
   "source": [
    "## Skew finding and mitigation strategies\n",
    "\n",
    "Salting\n",
    "\n",
    "Broadcast join - if broadcast join is applied on the skewed columns then AQE switches to sort merge join in runtime\n",
    "\n",
    "Range partitioning - range partitioning can be used to minimize partition scans but it can make skew worse if single range partition contains skewed data\n",
    "\n",
    "Custom partitioner (high-level pseudocode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b790e",
   "metadata": {},
   "source": [
    "| Strategy | When it works best | When it fails / limitations | Cost / Downside |\n",
    "|----------|--------------------|-----------------------------|------------------|\n",
    "| Salting | When a small set of join/aggregation keys (hot keys) causes skew — you add a salt to spread the heavy key across partitions. Example: single userId with 90% rows. | Fails if skew is due to many moderately-heavy keys (not a single hot key); does not help range query patterns. If not applied consistently it breaks deterministic joins. | Increases data size (extra column), increases shuffle & I/O, complicates downstream join logic (need to un-salt), requires careful key selection and testing. |\n",
    "| Range partitioning | When queries primarily filter by a numeric/date range and the distribution across ranges is reasonably even and stable. Good for time-windowed reads (time-series). | Fails when data distribution is uneven or changes over time (hot ranges), and for queries that are not range-based (point lookups or joins on other keys). Repartitioning for new ranges is costly. | Can create hotspots (heavy partitions), expensive re-partitioning/maintenance, can produce many small/imbalanced files, requires knowledge of data distribution. |\n",
    "| Custom Partitioner (hash/rule-based) | When you can design a deterministic function that routes keys to balanced partitions (e.g., composite key hashing) and you control producer & consumer logic. | Fails when query patterns change or when other joins use different partition keys; harder to apply with high-level DataFrame APIs (more natural with RDDs). | Higher implementation complexity, maintenance cost, risk of creating new imbalance if partitioner is poor; may require custom serialization/coordination. |\n",
    "| Broadcast Join | When one side of the join is small enough to fit comfortably in executor memory (and is stable). Fastest fix for many join skews. | Fails when the \"small\" side grows unexpectedly or when memory limits are restrictive; not suitable for large-scale joins. | Memory pressure on executors, OOM risk; increases driver/driver-to-executor broadcast overhead; not applicable to multi-stage complex joins. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8e5147",
   "metadata": {},
   "source": [
    "##### Salting: \n",
    "             adding salt = randInt(0, N) and join on (key, salt).\n",
    "add salt: only salt the hot key (fast)\n",
    "salted = inputdf.withColumn(\n",
    "    \"salt\",\n",
    "    F.when(F.col(\"skew_key\") == \"0\", (F.rand() * k).cast(\"int\")).otherwise(F.lit(0))\n",
    ")\n",
    ".withColumn(\"salted_key\", F.concat_ws(\"_\", F.col(\"skew_key\"), F.col(\"salt\")))\n",
    "\n",
    "\n",
    "##### Range partitioning: \n",
    "               df.write.partitionBy(\"year\",\"month\") \n",
    "               hotspot case: If most of the data falls under a single month of a perticular year then range partitioning is not useful\n",
    "\n",
    "##### Custom partitioner: \n",
    "                RDD.partitionBy(customPartitioner)\n",
    "                  mentioned in below cells\n",
    "\n",
    "##### Broadcast: \n",
    "                broadcast(small_df) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d674f2",
   "metadata": {},
   "source": [
    "### One quick decision flow (one-liner):\n",
    "\n",
    "If small side < memory → broadcast. Else if single hot key → salt. Else if queries are range-driven and stable → range partition. Else consider custom partitioner or redesign schema.\n",
    "\n",
    "One interview-ready metric to show: e.g., “Before: job shuffle read 800MB, runtime 12m. After salting: shuffle read 600MB, runtime 7m (but write size +100MB).” Even hypothetical numbers are better than no numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9768e49",
   "metadata": {},
   "source": [
    "## Custom Partition"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76a95283",
   "metadata": {},
   "source": [
    "1️⃣ Range-based custom partitioner\n",
    "\n",
    "For example: put small values in partition 0, mid values in 1, large values in 2.\n",
    "\n",
    "def my_partitioner(key):\n",
    "    k = int(key)\n",
    "    if k < 100:       return 0\n",
    "    elif k < 1000:    return 1\n",
    "    else:             return 2\n",
    "\n",
    "\n",
    "Spark’s default hash partitioner cannot do this.\n",
    "It doesn't know your data semantics.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b0a4a7c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "2️⃣ Load-balanced partitioner\n",
    "\n",
    "Let’s say key 0 is too hot for 1 partition.\n",
    "You can manually split it:\n",
    "\n",
    "def my_partitioner(key):\n",
    "    if key == \"0\":\n",
    "        return random.randint(0, 7)   # split key 0 into 8 partitions\n",
    "    return (hash(key) % 24) + 8        # put all other keys into partitions 8–31\n",
    "\n",
    "\n",
    "Now:\n",
    "\n",
    "key \"0\" → goes to partitions 0–7\n",
    "\n",
    "all other keys → go to 8–31\n",
    "\n",
    "This is equivalent to salting, but with deterministic control at the partitioner-level.\n",
    "\n",
    "Spark's default hash can't do this.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d9b9302",
   "metadata": {},
   "source": [
    "\n",
    "3️⃣ Colocation partitioner for joins\n",
    "\n",
    "If you know certain keys must always land in the same partition across RDDs:\n",
    "\n",
    "def my_partitioner(key):\n",
    "    important_ranges = [10,11,12,13]\n",
    "    if key in important_ranges:\n",
    "        return 0\n",
    "    return hash(key) % 31\n",
    "\n",
    "\n",
    "Spark’s default cannot selectively colocate only some keys.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a76467",
   "metadata": {},
   "source": [
    "## The BIG limitation: DataFrames do NOT support custom partition functions\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5dbd68f",
   "metadata": {},
   "source": [
    "\n",
    "Unlike RDDs, you cannot do this in a DataFrame:\n",
    "\n",
    "df.repartition(32, my_custom_partitioner)\n",
    "\n",
    "\n",
    "or anything like:\n",
    "\n",
    "df.partitionBy(my_custom_logic)\n",
    "\n",
    "\n",
    "Spark SQL / DataFrames do NOT allow you to plug in a custom partitioning function.\n",
    "\n",
    "You can only do:\n",
    "\n",
    "✔ Hash partitioning\n",
    "\n",
    "df.repartition(32, \"column\")\n",
    "\n",
    "\n",
    "✔ Range partitioning\n",
    "\n",
    "df.repartitionByRange(32, \"column\")\n",
    "\n",
    "\n",
    "✔ Round robin\n",
    "\n",
    "df.repartition(32)\n",
    "\n",
    "\n",
    "That’s it.\n",
    "\n",
    "No fully custom logic (like splitting hot keys, or complex rules like “put keys 0,1,2 in partition 0 and everything else evenly across others”).\n",
    "Spark simply doesn’t support that at the DataFrame level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff66c196",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e06479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, length\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e077060",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"skewData\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8a894e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdf = spark.read.parquet(\"E:/projects/DataEngineeringPrep/skew_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0267d1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_id: string (nullable = true)\n",
      " |-- skew_key: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9b0b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdf = inputdf.withColumn(\"amount_value\", F.col(\"amount\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d709eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|skew_key|sum(amount_value)|\n",
      "+--------+-----------------+\n",
      "|       0|        449680279|\n",
      "|     182|            70313|\n",
      "|     233|            69457|\n",
      "|     163|            66097|\n",
      "|     151|            65781|\n",
      "|     996|            64701|\n",
      "|      76|            64362|\n",
      "|     581|            64079|\n",
      "|     180|            64056|\n",
      "|     972|            64024|\n",
      "|      95|            63957|\n",
      "|     876|            63434|\n",
      "|     648|            63204|\n",
      "|     443|            63168|\n",
      "|     981|            62681|\n",
      "|     864|            62572|\n",
      "|     425|            62423|\n",
      "|     449|            62242|\n",
      "|     333|            62242|\n",
      "|     819|            62198|\n",
      "+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputdf.groupBy(\"skew_key\").sum(\"amount_value\").orderBy(F.desc(\"sum(amount_value)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf365fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://DESKTOP-AGBO2U2:4040\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e0ef0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [row_id#320, skew_key#321, amount#322, category#323, cast('amount as int) AS amount_value#328]\n",
      "+- Relation [row_id#320,skew_key#321,amount#322,category#323] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "row_id: string, skew_key: string, amount: string, category: string, amount_value: int\n",
      "Project [row_id#320, skew_key#321, amount#322, category#323, cast(amount#322 as int) AS amount_value#328]\n",
      "+- Relation [row_id#320,skew_key#321,amount#322,category#323] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [row_id#320, skew_key#321, amount#322, category#323, cast(amount#322 as int) AS amount_value#328]\n",
      "+- Relation [row_id#320,skew_key#321,amount#322,category#323] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [row_id#320, skew_key#321, amount#322, category#323, cast(amount#322 as int) AS amount_value#328]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [row_id#320,skew_key#321,amount#322,category#323] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/E:/projects/DataEngineeringPrep/skew_data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<row_id:string,skew_key:string,amount:string,category:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputdf.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46499772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Project (3)\n",
      "+- * ColumnarToRow (2)\n",
      "   +- Scan parquet  (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [4]: [row_id#320, skew_key#321, amount#322, category#323]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/E:/projects/DataEngineeringPrep/skew_data]\n",
      "ReadSchema: struct<row_id:string,skew_key:string,amount:string,category:string>\n",
      "\n",
      "(2) ColumnarToRow [codegen id : 1]\n",
      "Input [4]: [row_id#320, skew_key#321, amount#322, category#323]\n",
      "\n",
      "(3) Project [codegen id : 1]\n",
      "Output [5]: [row_id#320, skew_key#321, amount#322, category#323, cast(amount#322 as int) AS amount_value#328]\n",
      "Input [4]: [row_id#320, skew_key#321, amount#322, category#323]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputdf.explain(mode = \"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c44ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|skew_key|count |\n",
      "+--------+------+\n",
      "|0       |900273|\n",
      "|182     |138   |\n",
      "|163     |128   |\n",
      "|253     |128   |\n",
      "|560     |127   |\n",
      "|581     |125   |\n",
      "|233     |125   |\n",
      "|648     |125   |\n",
      "|207     |124   |\n",
      "|654     |124   |\n",
      "|996     |124   |\n",
      "|138     |123   |\n",
      "|425     |123   |\n",
      "|180     |123   |\n",
      "|970     |122   |\n",
      "|288     |122   |\n",
      "|498     |122   |\n",
      "|352     |121   |\n",
      "|657     |121   |\n",
      "|864     |121   |\n",
      "+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputdf.groupBy(\"skew_key\").count().orderBy(F.desc(\"count\")).show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b360f7",
   "metadata": {},
   "source": [
    "Salting\n",
    "\n",
    "Broadcast join\n",
    "\n",
    "Range partitioning\n",
    "\n",
    "Custom partitioner (high-level pseudocode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a74ec203",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdf = inputdf.withColumn(\"salted_key\", F.floor(F.rand()*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "43de0e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+--------+------------+----------+\n",
      "|row_id|skew_key|            amount|category|amount_value|salted_key|\n",
      "+------+--------+------------------+--------+------------+----------+\n",
      "|     0|       0| 669.3620514380217|     hot|         669|       118|\n",
      "|     1|       0| 776.4620725770643|     hot|         776|       910|\n",
      "|     2|       0|367.75575002118535|     hot|         367|       473|\n",
      "|     3|       0| 494.3132858506487|     hot|         494|       844|\n",
      "|     4|       0| 591.7821226474998|     hot|         591|       309|\n",
      "|     5|       0| 632.1642964005171|     hot|         632|         4|\n",
      "|     6|       0|32.164495567189256|     hot|          32|       664|\n",
      "|     7|       0|50.587504177523336|     hot|          50|       171|\n",
      "|     8|       0| 961.6080597124873|     hot|         961|       640|\n",
      "|     9|       0|182.88575075928327|     hot|         182|       344|\n",
      "|    10|       0| 598.8453075088435|     hot|         598|        86|\n",
      "|    11|       0|205.17810346800758|     hot|         205|       430|\n",
      "|    12|       0| 821.2032583765879|     hot|         821|        98|\n",
      "|    13|       0|356.82660625337735|     hot|         356|       752|\n",
      "|    14|       0| 841.8207255030852|     hot|         841|       652|\n",
      "|    15|       0| 899.7718159902983|     hot|         899|       634|\n",
      "|    16|       0| 516.8778193829008|     hot|         516|       597|\n",
      "|    17|       0| 577.4995652114022|     hot|         577|        19|\n",
      "|    18|       0| 913.7938873367087|     hot|         913|       219|\n",
      "|    19|       0| 590.2536025464301|     hot|         590|       788|\n",
      "+------+--------+------------------+--------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "45fd531e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|salted_key|sum(amount_value)|\n",
      "+----------+-----------------+\n",
      "|       474|           506083|\n",
      "|       964|           520770|\n",
      "|        29|           496408|\n",
      "|        26|           538848|\n",
      "|       418|           505654|\n",
      "|       191|           502832|\n",
      "|       558|           518881|\n",
      "|       541|           492404|\n",
      "|        65|           498805|\n",
      "|       293|           470204|\n",
      "|       222|           490322|\n",
      "|       270|           521946|\n",
      "|       938|           463134|\n",
      "|       730|           514772|\n",
      "|       442|           537563|\n",
      "|       705|           499682|\n",
      "|       243|           525766|\n",
      "|       720|           506541|\n",
      "|       367|           511906|\n",
      "|       278|           496530|\n",
      "+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputdf.groupBy(\"salted_key\").sum(\"amount_value\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5e438ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Project (3)\n",
      "+- * ColumnarToRow (2)\n",
      "   +- Scan parquet  (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [4]: [row_id#320, skew_key#321, amount#322, category#323]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/E:/projects/DataEngineeringPrep/skew_data]\n",
      "ReadSchema: struct<row_id:string,skew_key:string,amount:string,category:string>\n",
      "\n",
      "(2) ColumnarToRow [codegen id : 1]\n",
      "Input [4]: [row_id#320, skew_key#321, amount#322, category#323]\n",
      "\n",
      "(3) Project [codegen id : 1]\n",
      "Output [6]: [row_id#320, skew_key#321, amount#322, category#323, cast(amount#322 as int) AS amount_value#328, FLOOR((rand(-3118830139136299688) * 1000.0)) AS salted_key#377L]\n",
      "Input [4]: [row_id#320, skew_key#321, amount#322, category#323]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputdf.explain(mode = \"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e3f243",
   "metadata": {},
   "source": [
    "### Broadcast join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd1d96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"E:/projects/DataEngineeringPrep/skew_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d5786f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "brodcastdf = F.broadcast(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53470f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "joineddf = inputdf.join(brodcastdf, \"skew_key\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8c62545d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (9)\n",
      "+- Project (8)\n",
      "   +- BroadcastHashJoin Inner BuildRight (7)\n",
      "      :- Filter (3)\n",
      "      :  +- Project (2)\n",
      "      :     +- Scan parquet  (1)\n",
      "      +- BroadcastExchange (6)\n",
      "         +- Filter (5)\n",
      "            +- Scan parquet  (4)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [4]: [row_id#320, skew_key#321, amount#322, category#323]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/E:/projects/DataEngineeringPrep/skew_data]\n",
      "ReadSchema: struct<row_id:string,skew_key:string,amount:string,category:string>\n",
      "\n",
      "(2) Project\n",
      "Output [6]: [row_id#320, skew_key#321, amount#322, category#323, cast(amount#322 as int) AS amount_value#328, FLOOR((rand(-3118830139136299688) * 1000.0)) AS salted_key#377L]\n",
      "Input [4]: [row_id#320, skew_key#321, amount#322, category#323]\n",
      "\n",
      "(3) Filter\n",
      "Input [6]: [row_id#320, skew_key#321, amount#322, category#323, amount_value#328, salted_key#377L]\n",
      "Condition : isnotnull(skew_key#321)\n",
      "\n",
      "(4) Scan parquet \n",
      "Output [4]: [row_id#432, skew_key#433, amount#434, category#435]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/E:/projects/DataEngineeringPrep/skew_data]\n",
      "PushedFilters: [IsNotNull(skew_key)]\n",
      "ReadSchema: struct<row_id:string,skew_key:string,amount:string,category:string>\n",
      "\n",
      "(5) Filter\n",
      "Input [4]: [row_id#432, skew_key#433, amount#434, category#435]\n",
      "Condition : isnotnull(skew_key#433)\n",
      "\n",
      "(6) BroadcastExchange\n",
      "Input [4]: [row_id#432, skew_key#433, amount#434, category#435]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[1, string, false]),false), [plan_id=706]\n",
      "\n",
      "(7) BroadcastHashJoin\n",
      "Left keys [1]: [skew_key#321]\n",
      "Right keys [1]: [skew_key#433]\n",
      "Join condition: None\n",
      "\n",
      "(8) Project\n",
      "Output [9]: [skew_key#321, row_id#320, amount#322, category#323, amount_value#328, salted_key#377L, row_id#432, amount#434, category#435]\n",
      "Input [10]: [row_id#320, skew_key#321, amount#322, category#323, amount_value#328, salted_key#377L, row_id#432, skew_key#433, amount#434, category#435]\n",
      "\n",
      "(9) AdaptiveSparkPlan\n",
      "Output [9]: [skew_key#321, row_id#320, amount#322, category#323, amount_value#328, salted_key#377L, row_id#432, amount#434, category#435]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in this case broadcast join worked dataset is too small and could easily fit below the broadcast thresold of 10Mb\n",
    "joineddf.explain(mode = \"formatted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
